{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#clear directory \n",
    "!rm -r ./YahooNews/YahooNewsQualityDataset-v0.8\n",
    "!rm ./Kaggle/fake.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import zipfile\n",
    "zip_ref1 = zipfile.ZipFile('./YahooNews/YahooNewsQualityDataset-v0.8.zip', 'r')\n",
    "zip_ref1.extractall('./YahooNews/')\n",
    "zip_ref1.close()\n",
    "\n",
    "\n",
    "zip_ref2 = zipfile.ZipFile('./Kaggle/fake-news.zip', 'r')\n",
    "zip_ref2.extractall('./Kaggle/')\n",
    "zip_ref2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotators.xlsx  excel_dataset.py  news.tsv\n",
      "dataset\t\t methodology\t   sentences.tsv\n",
      "fake-news.zip  fake.csv\n"
     ]
    }
   ],
   "source": [
    "!ls ./YahooNews/YahooNewsQualityDataset-v0.8\n",
    "!ls ./Kaggle/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Created on 29/10/2016\n",
    "\n",
    "@author: PelejaF\n",
    "'''\n",
    "\n",
    "import os\n",
    "folder = 'YahooNews/YahooNewsQualityDataset-v0.8/'\n",
    "news_tsv_folder = os.path.join(os.getcwd(),folder+'dataset/tsvFiles/withText-body/')\n",
    "sentences_tsv_folder = os.path.join(os.getcwd(),folder+'dataset/tsvFiles/withText-sentences/')\n",
    "\n",
    "news_csv = os.path.join(os.getcwd(),'news.csv')\n",
    "sentences_csv = os.path.join(os.getcwd(),'sentences.csv')\n",
    "\n",
    "news_csv_out = open(news_csv, 'w')\n",
    "sentences_csv_out = open(sentences_csv, 'w')\n",
    "\n",
    "# write headers\n",
    "news_csv_out.write('id\\tnews article name\\tnews article\\tFluency\\tConciseness\\tDescriptiveness\\tNovelty\\tCompleteness\\t')\n",
    "news_csv_out.write('Referencing\\tFormality\\tRichness\\tAttractiveness\\tTechnicality\\tPopularity\\tSubjectivity\\t')\n",
    "news_csv_out.write('Positive Emotion\\tNegative Emotion\\tQuality\\tAnnotatorsConfidenceScore\\n')\n",
    "\n",
    "news_article_names = {}\n",
    "\n",
    "curr_id = 1 \n",
    "for filename in os.listdir(news_tsv_folder):\n",
    "\tcurr_file = open(os.path.join(news_tsv_folder, filename), 'r')\n",
    "\tline = curr_file.readline().replace('\\n','').replace('\\r','')\n",
    "\tline = line.split('\\t')\n",
    "\tnews_article_name = filename.replace('-sentences.txt','').replace('_sentences.txt','').replace('-body.txt','').replace('_body.txt','')\n",
    "\tnews_article_names[news_article_name] = curr_id\n",
    "\tnews_article = line[0]\n",
    "\tFluency = line[1]\n",
    "\tConciseness = line[2]\n",
    "\tDescriptiveness = line[3]\n",
    "\tNovelty = line[4]\n",
    "\tCompleteness = line[5]\n",
    "\tReferencing = line[6]\n",
    "\tFormality = line[7]\n",
    "\tRichness = line[8]\n",
    "\tAttractiveness = line[9]\n",
    "\tTechnicality = line[10]\n",
    "\tPopularity = line[11]\n",
    "\tSubjectivity = line[12]\n",
    "\tPositive_Emotion = line[13]\n",
    "\tNegative_Emotion = line[14]\n",
    "\tQuality = line[15]\n",
    "\tif len(line) == 17:\n",
    "\t\tAnnotatorsConfidenceScore = line[16]\n",
    "\telse:\n",
    "\t\tAnnotatorsConfidenceScore = ''\n",
    "\tnews_csv_out.write(str(curr_id)+'\\t'+news_article_name+'\\t'+news_article+'\\t'+Fluency+'\\t'+Conciseness+'\\t'+Descriptiveness+'\\t'+Novelty+'\\t'+Completeness+'\\t')\n",
    "\tnews_csv_out.write(Referencing+'\\t'+Formality+'\\t'+Richness+'\\t'+Attractiveness+'\\t'+Technicality+'\\t')\n",
    "\tnews_csv_out.write(Popularity+'\\t'+Subjectivity+'\\t'+Positive_Emotion+'\\t'+Negative_Emotion+'\\t'+Quality+'\\t'+AnnotatorsConfidenceScore+'\\n')\n",
    "\tcurr_id = curr_id + 1\n",
    "\tcurr_file.close()\n",
    "news_csv_out.close()\n",
    "\n",
    "# write headers\n",
    "sentences_csv_out.write('id\\tnews article name\\tSentence\\tSubjectivity\\tPositivity\\tNegativity\\t')\n",
    "sentences_csv_out.write('Ignore sentence (I) or sentence Splitting issue (S)\\tAnnotatorsConfidenceScore\\n')\n",
    "for filename in os.listdir(sentences_tsv_folder):\n",
    "\tcurr_file = open(os.path.join(sentences_tsv_folder, filename), 'r')\n",
    "\tnews_article_name = filename.replace('-sentences.txt','').replace('_sentences.txt','').replace('-body.txt','').replace('_body.txt','')\n",
    "\tcurr_id = news_article_names[news_article_name]\n",
    "\twhile 1:\t\n",
    "\t\tsentence = curr_file.readline()\n",
    "\t\tif not sentence:break\n",
    "\t\tline = sentence.replace('\\n','').replace('\\r','').split('\\t')\n",
    "\t\tif len(line) == 1:break\n",
    "\t\tnews_article = line[0]\n",
    "\t\tsentence = line[1]\n",
    "\t\tsubjectivity = line[2]\n",
    "\t\tpositivity = line[3]\n",
    "\t\tnegativity = line[4]\n",
    "\t\tif len(line) == 6:\n",
    "\t\t\tI_S = line[5]\n",
    "\t\telse:\n",
    "\t\t\tI_S = ''\n",
    "\t\tif len(line) == 7:\n",
    "\t\t\tannotatorsConfidenceScore = line[6]\n",
    "\t\telse:\n",
    "\t\t\tannotatorsConfidenceScore = ''\n",
    "\t\tsentences_csv_out.write(str(curr_id)+'\\t'+news_article_name+'\\t'+news_article+'\\t'+sentence+'\\t'+subjectivity+'\\t')\n",
    "\t\tsentences_csv_out.write(positivity+'\\t'+negativity+'\\t'+I_S+'\\t'+annotatorsConfidenceScore+'\\n')\n",
    "\tcurr_file.close()\n",
    "sentences_csv_out.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "header = []\n",
    "articleData = []\n",
    "with open('news.csv', 'rb') as f:\n",
    "    for line in f:\n",
    "        if len(header) == 0:\n",
    "            header = line.split('\\t')\n",
    "        else:\n",
    "            articleData.append(line.split('\\t'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "658 19\n",
      "['id', 'news article name', 'news article', 'Fluency', 'Conciseness', 'Descriptiveness', 'Novelty', 'Completeness', 'Referencing', 'Formality', 'Richness', 'Attractiveness', 'Technicality', 'Popularity', 'Subjectivity', 'Positive Emotion', 'Negative Emotion', 'Quality', 'AnnotatorsConfidenceScore\\n']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "columns = len(header)\n",
    "rows = len(articleData)\n",
    "print rows, columns\n",
    "print header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['346', 'park-defends-refusal-wolves-184423303', \"BOULDER, Colo. (AP) \\xe2\\x80\\x94 The National Park Service acted properly when it ruled out using wolves to control the elk population in Rocky Mountain National Park, government lawyers argued Thursday before a federal appeals court.The government also defended the use of trained volunteers to help Park Service employees shoot and kill excess elk, saying it didn't violate a hunting ban in national parks.In a hearing before the 10th U.S. Circuit Court of Appeals, a law student representing the wildlife advocacy group WildEarth Guardians argued the Park Service did not give enough consideration to the wolf option and rejected it without giving the public a chance to comment.The group also said letting volunteers shoot elk instead of limiting the shooting to Park Service employees was tantamount to hunting.WildEarth Guardians sued the Park Service in 2008, asking a Denver federal judge to overturn the park's elk-thinning policy. The judge upheld the policy last year, and WildEarth Guardians appealed to the 10th Circuit.The appeals court normally meets in Denver but heard this case at the University of Colorado Law School in Boulder as part of an outreach program. The judges did not say when they would rule.Rocky Mountain National Park sometimes has so many elk that they overgraze the vegetation, leaving other animals without enough food and habitat. Few natural predators are left there, and hunting is prohibited, so little remains to keep the elk population in check.The park launched a 20-year program in 2008 to thin the herd by having park employees and trained volunteers under park supervision periodically shoot and kill elk. The program also includes fences to protect vegetation from elk and redistributing some of the animals.Officials said reintroducing wolves to control elk numbers was infeasible. They cited a lack of support from other agencies, safety concerns of people who live nearby, expected conflicts between wolves and humans and the amount of attention that a wolf population would require of park officials.Park spokeswoman Kyle Patterson said 131 elk have been killed in the culling program during since 2008.The current size of the herd in the park's lower elevations is 600 to 800, which is within the target range set by the program, so no elk were killed last winter.No decision has been made on whether or how many elk will be killed this coming winter.Wolves disappeared from much of the West after decades of hunting and government-backed extermination. They were re-introduced in Yellowstone National Park in Wyoming in 1995, and some advocates have argued for bringing them back elsewhere.___Follow Dan Elliott at http://twitter.com/DanElliottAP___Online:National Park Service fact sheet: http://1.usa.gov/UllcuKWildEarth Guardians: http://bit.ly/T94CDm\", '4', '3', '3', '3', '3', '3', '2', '3', '3', '2', '3', '2', '1', '2', '3', '4\\n']\n"
     ]
    }
   ],
   "source": [
    "print articleData[345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[226, 642, 343, 435, 184, 139, 18, 218, 493, 302, 192, 344, 213, 39, 606, 472, 597, 571, 628, 117, 75, 10, 562, 515, 566, 15, 484, 558, 458, 594, 58, 553, 266, 168, 261, 375, 307, 148, 288, 353, 468, 464, 408, 197, 511, 141, 225, 260, 630, 634, 415, 22, 583, 187, 273, 264, 508, 495, 255, 442, 394, 504, 222, 102, 426, 179, 190, 171, 212, 469, 252, 146, 200, 8, 595, 322, 77, 453, 582, 622, 577, 454, 147, 250, 152, 219, 531, 291, 239, 49, 83, 280, 305, 601, 154, 201, 565, 467, 512, 471, 185, 269, 107, 215, 181, 283, 417, 345, 587, 357, 487, 368, 53, 397, 243, 257, 434, 71, 535, 412, 366, 542, 578, 330, 298, 199, 457, 499, 551, 427, 590]\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "rowid = range(0, rows)\n",
    "shuffle(rowid)\n",
    "trainid = []\n",
    "testid = []\n",
    "ratio = 0.8\n",
    "for i in range(0, rows):\n",
    "    if i < rows * ratio:\n",
    "        trainid.append(rowid[i])\n",
    "    else:\n",
    "        testid.append(rowid[i])\n",
    "\n",
    "print testid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data from yahoo dataset\n",
    "def load_yahoo_data(label_used):\n",
    "    header = []\n",
    "    articleData = []\n",
    "    with open('./news.csv', 'rb') as f:\n",
    "        for line in f:\n",
    "            if len(header) == 0:\n",
    "                header = line.split('\\t')\n",
    "            else:\n",
    "                articleData.append(line.split('\\t'))\n",
    "    columns = len(header)\n",
    "    rows = len(articleData)\n",
    "    print 'Loaded rows: ', rows,'columns: ', columns\n",
    "    print 'Headers: ', header\n",
    "    print 'Using label: ', label_used\n",
    "    #print 'sample text: ', articleData[0][header.index('news article')]\n",
    "    #print 'sample label: ', articleData[0][header.index(label_used)]\n",
    "    \n",
    "    texts = []\n",
    "    targets = []\n",
    "    for i in range(0, rows):\n",
    "        labelStr = articleData[i][header.index(label_used)]\n",
    "        if labelStr != '':\n",
    "            texts.append(articleData[i][header.index('news article')])\n",
    "            target = int(labelStr)\n",
    "            labels = [[0,1]]\n",
    "            if target <= 3:\n",
    "                labels = [[1,0]]\n",
    "            targets.append(labels)\n",
    "    y = np.concatenate(targets, 0)\n",
    "    return (texts, y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows:  658 columns:  19\n",
      "Headers:  ['id', 'news article name', 'news article', 'Fluency', 'Conciseness', 'Descriptiveness', 'Novelty', 'Completeness', 'Referencing', 'Formality', 'Richness', 'Attractiveness', 'Technicality', 'Popularity', 'Subjectivity', 'Positive Emotion', 'Negative Emotion', 'Quality', 'AnnotatorsConfidenceScore\\n']\n",
      "Using label:  Fluency\n",
      "[[0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " ..., \n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "(texts, targets) = load_yahoo_data('Fluency')\n",
    "print targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=64\n",
      "CHECKPOINT_EVERY=20\n",
      "DEV_SAMPLE_PERCENTAGE=0.1\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=20\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "LABEL_USED=Formality\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=40\n",
      "NUM_FILTERS=128\n",
      "UNROLLED_LSTM=False\n",
      "\n",
      "Loading data...\n",
      "Loaded rows:  658 columns:  19\n",
      "Headers:  ['id', 'news article name', 'news article', 'Fluency', 'Conciseness', 'Descriptiveness', 'Novelty', 'Completeness', 'Referencing', 'Formality', 'Richness', 'Attractiveness', 'Technicality', 'Popularity', 'Subjectivity', 'Positive Emotion', 'Negative Emotion', 'Quality', 'AnnotatorsConfidenceScore\\n']\n",
      "Using label:  Formality\n",
      "Vocabulary Size: 28958\n",
      "Train/Dev split: 593/65\n",
      "Writing to /notebooks/Project/runs/1490685741\n",
      "\n",
      "2017-03-28T07:22:29.708440: step 1, loss 3.07313, acc 0.515625\n",
      "2017-03-28T07:22:36.393552: step 2, loss 1.70263, acc 0.671875\n",
      "2017-03-28T07:22:42.804019: step 3, loss 0.646374, acc 0.921875\n",
      "2017-03-28T07:22:48.984269: step 4, loss 1.27697, acc 0.875\n",
      "2017-03-28T07:22:55.140097: step 5, loss 1.58074, acc 0.828125\n",
      "2017-03-28T07:23:01.366528: step 6, loss 1.98248, acc 0.8125\n",
      "2017-03-28T07:23:07.614113: step 7, loss 1.53542, acc 0.90625\n",
      "2017-03-28T07:23:13.734825: step 8, loss 2.26992, acc 0.796875\n",
      "2017-03-28T07:23:19.961934: step 9, loss 1.89008, acc 0.84375\n",
      "2017-03-28T07:23:21.594693: step 10, loss 0.643425, acc 0.941176\n",
      "2017-03-28T07:23:27.762381: step 11, loss 1.52783, acc 0.8125\n",
      "2017-03-28T07:23:33.916053: step 12, loss 0.82737, acc 0.8125\n",
      "2017-03-28T07:23:40.170843: step 13, loss 0.657107, acc 0.859375\n",
      "2017-03-28T07:23:46.559998: step 14, loss 1.82889, acc 0.765625\n",
      "2017-03-28T07:23:52.896234: step 15, loss 1.51433, acc 0.796875\n",
      "2017-03-28T07:23:59.420891: step 16, loss 1.64843, acc 0.671875\n",
      "2017-03-28T07:24:05.800934: step 17, loss 1.58201, acc 0.703125\n",
      "2017-03-28T07:24:12.083527: step 18, loss 2.02088, acc 0.625\n",
      "2017-03-28T07:24:18.137958: step 19, loss 1.28666, acc 0.765625\n",
      "2017-03-28T07:24:19.790685: step 20, loss 0.279479, acc 0.882353\n",
      "\n",
      "Evaluation:\n",
      "2017-03-28T07:24:21.996417: step 20, loss 1.37521, acc 0.830769\n",
      "\n",
      "Saved model checkpoint to /notebooks/Project/runs/1490685741/checkpoints/model-20\n",
      "\n",
      "2017-03-28T07:24:28.613577: step 21, loss 0.938932, acc 0.8125\n",
      "2017-03-28T07:24:34.961933: step 22, loss 0.929103, acc 0.828125\n",
      "2017-03-28T07:24:41.081853: step 23, loss 0.533655, acc 0.90625\n",
      "2017-03-28T07:24:47.443176: step 24, loss 1.28959, acc 0.828125\n",
      "2017-03-28T07:24:53.727346: step 25, loss 2.38966, acc 0.703125\n",
      "2017-03-28T07:25:00.073064: step 26, loss 0.972848, acc 0.875\n",
      "2017-03-28T07:25:06.329799: step 27, loss 1.06941, acc 0.875\n",
      "2017-03-28T07:25:12.398252: step 28, loss 1.45818, acc 0.75\n",
      "2017-03-28T07:25:18.699748: step 29, loss 0.361853, acc 0.9375\n",
      "2017-03-28T07:25:20.369303: step 30, loss 1.38927, acc 0.705882\n",
      "2017-03-28T07:25:26.492918: step 31, loss 1.04993, acc 0.859375\n",
      "2017-03-28T07:25:32.591420: step 32, loss 0.965265, acc 0.8125\n",
      "2017-03-28T07:25:38.684738: step 33, loss 0.687032, acc 0.875\n",
      "2017-03-28T07:25:44.805328: step 34, loss 1.06335, acc 0.84375\n",
      "2017-03-28T07:25:50.923075: step 35, loss 0.414557, acc 0.875\n",
      "2017-03-28T07:25:57.015443: step 36, loss 1.12993, acc 0.765625\n",
      "2017-03-28T07:26:03.248028: step 37, loss 0.962585, acc 0.75\n",
      "2017-03-28T07:26:09.672733: step 38, loss 1.15089, acc 0.71875\n",
      "2017-03-28T07:26:15.936294: step 39, loss 0.924088, acc 0.78125\n",
      "2017-03-28T07:26:17.597154: step 40, loss 0.464046, acc 0.882353\n",
      "\n",
      "Evaluation:\n",
      "2017-03-28T07:26:19.664020: step 40, loss 1.207, acc 0.830769\n",
      "\n",
      "Saved model checkpoint to /notebooks/Project/runs/1490685741/checkpoints/model-40\n",
      "\n",
      "2017-03-28T07:26:26.327585: step 41, loss 0.799427, acc 0.796875\n",
      "2017-03-28T07:26:32.600131: step 42, loss 1.04209, acc 0.8125\n",
      "2017-03-28T07:26:38.918310: step 43, loss 0.895483, acc 0.859375\n",
      "2017-03-28T07:26:45.116475: step 44, loss 0.656859, acc 0.84375\n",
      "2017-03-28T07:26:51.394284: step 45, loss 1.03272, acc 0.8125\n",
      "2017-03-28T07:26:57.564710: step 46, loss 0.540119, acc 0.921875\n",
      "2017-03-28T07:27:03.775852: step 47, loss 0.462682, acc 0.90625\n",
      "2017-03-28T07:27:10.018189: step 48, loss 0.561072, acc 0.859375\n"
     ]
    }
   ],
   "source": [
    "!python ./shared_lib/train.py --label_used 'Formality'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
